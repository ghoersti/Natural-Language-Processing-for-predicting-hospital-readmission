{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Natural-Language-Processing-for-predicting-hospital-readmission( Azure)\n",
    "> This note book is intended to run on an azure HD insights cluster\n",
    "\n",
    "The following files must have been constructed(pre-Azure ) and uploaded to your HDINSIGHTS storage container\n",
    "\n",
    "```python\n",
    "'notes_discharge_pd.csv' , 'ADMISSIONS.CSV'  and 'IDC9_filter.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'kind': 'pyspark3', u'name': u'remotesparkmagics'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'kind': 'pyspark3', u'name': u'remotesparkmagics-sample', u'heartbeatTimeoutInSecond': 6000, u'driverMemory': u'28g', u'executorCores': 6, u'executorMemory': u'28G'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"28G\", \"executorCores\":6,\"driverMemory\":\"28g\", \"heartbeatTimeoutInSecond\":6000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>31</td><td>application_1541818620294_0035</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-cse625.zigv3mzdk4kurp5tm2yczuwtcf.cx.internal.cloudapp.net:8088/proxy/application_1541818620294_0035/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-cse625.zigv3mzdk4kurp5tm2yczuwtcf.cx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1541818620294_0035_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=yarn appName=remotesparkmagics-sample>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyspark.ml.feature \n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import pyspark.sql.functions  as psf\n",
    "from pyspark.sql import Window \n",
    "from pyspark.sql.types  import DateType\n",
    "#import pandas as pd\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType , DateType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType , DateType, ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext ,SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wasb[s]://cse6250-2018-11-10t02-36-55-285z@flufy_lobster69.blob.core.windows.net/<path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv containing more IDC diagnosis and procedure codes \n",
    "IDC9_filter=(spark.read\n",
    "    .option(\"inferschema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .csv(\"wasb:///IDC9_filter.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admissions_sdf=(spark.read\n",
    "    .option(\"inferschema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .csv(\"wasb:///ADMISSIONS.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note_struct = StructType([StructField(\"ROW_ID\", IntegerType(), True),\n",
    "#                       StructField(\"SUBJECT_ID\", IntegerType(), True),\n",
    "#                       StructField(\"HADM_ID\", StringType(), True),\n",
    "#                       StructField(\"CHARTDATE\", StringType(), True),\n",
    "#                       StructField(\"CHARTTIME\", StringType(), True),\n",
    "#                       StructField(\"STORETIME\", StringType(), True),\n",
    "#                       StructField(\"CATEGORY\", StringType(), True),\n",
    "#                       StructField(\"DESCRIPTION\", StringType(), True),\n",
    "#                       StructField(\"CGID\", StringType(), True),\n",
    "#                       StructField(\"ISERROR\", StringType(), True),\n",
    "#                       StructField(\"TEXT\", StringType(), True)])\n",
    "notes_sdf=(spark.read\n",
    "    .option(\"inferschema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \"|\")\n",
    "    .csv(\"wasb:///notes_discharge_pd.csv\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ROW_ID=21, SUBJECT_ID=22, HADM_ID=165315, ADMITTIME=datetime.datetime(2196, 4, 9, 12, 26), DISCHTIME=datetime.datetime(2196, 4, 10, 15, 54), DEATHTIME=None, ADMISSION_TYPE='EMERGENCY', ADMISSION_LOCATION='EMERGENCY ROOM ADMIT', DISCHARGE_LOCATION='DISC-TRAN CANCER/CHLDRN H', INSURANCE='Private', LANGUAGE=None, RELIGION='UNOBTAINABLE', MARITAL_STATUS='MARRIED', ETHNICITY='WHITE', EDREGTIME=datetime.datetime(2196, 4, 9, 10, 6), EDOUTTIME=datetime.datetime(2196, 4, 9, 13, 24), DIAGNOSIS='BENZODIAZEPINE OVERDOSE', HOSPITAL_EXPIRE_FLAG=0, HAS_CHARTEVENTS_DATA=1)]"
     ]
    }
   ],
   "source": [
    "admissions_sdf.cache().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import notes \n",
    "# notes_df = spark.read.csv(\"wasb:///NOTEEVENTS.csv\", header=True,schema=note_struct)\n",
    "#csvFile = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(SUBJECT_ID=22532, HADM_ID=167853.0, CATEGORY='Discharge summary', TEXT='Admission Date:  [**2151-7-16**]       Discharge Date:  [**2151-8-4**]   Service: ADDENDUM:  RADIOLOGIC STUDIES:  Radiologic studies also included a chest CT, which confirmed cavitary lesions in the left lung apex consistent with infectious process/tuberculosis.  This also moderate-sized left pleural effusion.  HEAD CT:  Head CT showed no intracranial hemorrhage or mass effect, but old infarction consistent with past medical history.  ABDOMINAL CT:  Abdominal CT showed lesions of T10 and sacrum most likely secondary to osteoporosis. These can be followed by repeat imaging as an outpatient.                                [**First Name8 (NamePattern2) **] [**First Name4 (NamePattern1) 1775**] [**Last Name (NamePattern1) **], M.D.  [**MD Number(1) 1776**]  Dictated By:[**Hospital 1807**] MEDQUIST36  D:  [**2151-8-5**]  12:11 T:  [**2151-8-5**]  12:21 JOB#:  [**Job Number 1808**] ')]"
     ]
    }
   ],
   "source": [
    "notes_sdf.cache().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('SUBJECT_ID', 'int'), ('HADM_ID', 'double'), ('CATEGORY', 'string'), ('TEXT', 'string')], [('ROW_ID', 'int'), ('SUBJECT_ID', 'int'), ('HADM_ID', 'int'), ('ADMITTIME', 'timestamp'), ('DISCHTIME', 'timestamp'), ('DEATHTIME', 'timestamp'), ('ADMISSION_TYPE', 'string'), ('ADMISSION_LOCATION', 'string'), ('DISCHARGE_LOCATION', 'string'), ('INSURANCE', 'string'), ('LANGUAGE', 'string'), ('RELIGION', 'string'), ('MARITAL_STATUS', 'string'), ('ETHNICITY', 'string'), ('EDREGTIME', 'timestamp'), ('EDOUTTIME', 'timestamp'), ('DIAGNOSIS', 'string'), ('HOSPITAL_EXPIRE_FLAG', 'int'), ('HAS_CHARTEVENTS_DATA', 'int')])"
     ]
    }
   ],
   "source": [
    "notes_sdf.dtypes, admissions_sdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset to used data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ADMITTIME: timestamp (nullable = true)\n",
      " |-- DISCHTIME: timestamp (nullable = true)\n",
      " |-- DEATHTIME: timestamp (nullable = true)\n",
      " |-- ADMISSION_TYPE: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- CATEGORY: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "#create list of used columns \n",
    "adm_cols=['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME',  'ADMISSION_TYPE']\n",
    "notes_cols=['SUBJECT_ID', 'HADM_ID', 'CATEGORY','TEXT' ]\n",
    "## subset notes and admissions to relavant data\n",
    "adm_used= admissions_sdf[adm_cols]\n",
    "\n",
    "\n",
    "\n",
    "notes_used = notes_sdf[notes_cols]\n",
    "# change key type \n",
    "notes_used = notes_used.select('SUBJECT_ID', psf.col('HADM_ID').cast('int').alias('HADM_ID'), 'CATEGORY','TEXT' ).cache()\n",
    "adm_used.printSchema()\n",
    "notes_used.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_adm=adm_used.orderBy( psf.col(\"SUBJECT_ID\"), psf.col('ADMITTIME'))\n",
    "w = Window.partitionBy(\"SUBJECT_ID\").orderBy(psf.desc('ADMITTIME'))\n",
    "#next_adm= sort_adm.withColumn('next_admit', psf.lag(col('ADMITTIME')).over(w))\n",
    "next_adm=sort_adm.select('*', psf.lag(psf.col('ADMITTIME')).over(w).alias('NEXT_ADMISSION')\n",
    "                , psf.lag(psf.col('ADMISSION_TYPE')).over(w).alias('NEXT_ADMISSION_TYPE')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+-------------------+--------------+-------------------+\n",
      "|SUBJECT_ID|HADM_ID|          ADMITTIME|NEXT_ADMISSION_TYPE|ADMISSION_TYPE|     NEXT_ADMISSION|\n",
      "+----------+-------+-------------------+-------------------+--------------+-------------------+\n",
      "|      4101| 161951|2103-01-22 18:01:00|           ELECTIVE|     EMERGENCY|2106-09-17 11:45:00|\n",
      "|      4900| 117413|2203-06-29 08:00:00|           ELECTIVE|      ELECTIVE|2203-10-26 13:15:00|\n",
      "+----------+-------+-------------------+-------------------+--------------+-------------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "#example\n",
    "next_adm\\\n",
    ".select('SUBJECT_ID','HADM_ID' ,'ADMITTIME' ,'NEXT_ADMISSION_TYPE','ADMISSION_TYPE','NEXT_ADMISSION')\\\n",
    ".where(psf.col(\"NEXT_ADMISSION\").isNotNull())\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add days to readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adm_datediff= next_adm.withColumn(\"days_next_admit\", psf.datediff(\"NEXT_ADMISSION\", \"DISCHTIME\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148"
     ]
    }
   ],
   "source": [
    "##example :) \n",
    "\n",
    "adm_datediff.take(1)[0]['SUBJECT_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         CATEGORY|\n",
      "+-----------------+\n",
      "|Discharge summary|\n",
      "+-----------------+"
     ]
    }
   ],
   "source": [
    "notes_used.select(\"CATEGORY\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter to discharge\n",
    "#notes_dis_sum = notes_used.filter( notes_used.CATEGORY == 'Discharge summary')\n",
    "## create index to dedup\n",
    "w =  Window.partitionBy(\"SUBJECT_ID\",'HADM_ID').orderBy(psf.desc('HADM_ID'))\n",
    "notes_dis_sum  = notes_used.withColumn(\"row_number\", psf.row_number().over(w)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## dedup\n",
    "notes_dis_dedup=notes_dis_sum.filter(notes_dis_sum.row_number==1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n",
      "|SUBJECT_ID|HADM_ID|count|\n",
      "+----------+-------+-----+\n",
      "|      1453| 122894|    2|\n",
      "|      1616| 176986|    2|\n",
      "|      2265| 100548|    2|\n",
      "+----------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-------+-----+\n",
      "|SUBJECT_ID|HADM_ID|count|\n",
      "+----------+-------+-----+\n",
      "+----------+-------+-----+"
     ]
    }
   ],
   "source": [
    "notes_dis_sum.groupBy(\"SUBJECT_ID\",'HADM_ID').count().filter(psf.col('count')>1).show(3)\n",
    "notes_dis_dedup.groupBy(\"SUBJECT_ID\",'HADM_ID').count().filter(psf.col('count')>1).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join notes and admisions remove newborns \n",
    "note_admit_join=adm_datediff\\\n",
    ".filter(psf.col('ADMISSION_TYPE')!='NEWBORN').join(notes_dis_dedup, ['HADM_ID','SUBJECT_ID'],'left').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.44102685838307"
     ]
    }
   ],
   "source": [
    "### check to see if cutoff of 30 is reasonable\n",
    "sum_between= adm_datediff.select('days_next_admit').groupBy().sum().take(1)[0]['sum(days_next_admit)']\n",
    "count_between= adm_datediff.select('days_next_admit').count()\n",
    "sum_between/count_between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create label\n",
    "note_admit_label=note_admit_join.select('*', psf.when(psf.col('days_next_admit')<30,1).otherwise(0).alias('label')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "# def tokenizer_better(text):\n",
    "#     # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "#     if text != None:\n",
    "#         punc_list = string.punctuation+'0123456789'\n",
    "#         t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "#         text = text.lower().translate(t)\n",
    "#         tokens = word_tokenize(text)\n",
    "#     else:\n",
    "#         tokens = []\n",
    "#     return tokens\n",
    "#spark_tokenizer = psf.udf(lambda text: tokenizer_better(text), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to clean\n",
    "import string\n",
    "def text_cleaner(text):\n",
    "    # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "    if text != None:\n",
    "        punc_list = string.punctuation+'0123456789'\n",
    "        t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "        text = text.lower().translate(t)\n",
    "        #tokens = word_tokenize(text)\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "#register UDF\n",
    "\n",
    "spark_cleaner= psf.udf(lambda text: text_cleaner(text),StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet_low = []\n",
    "for letter in range(97,123):\n",
    "    alphabet_low.append(chr(letter))\n",
    "stopwords=['','it', 'did', 'he', 'mustn', 'whom', \"that'll\", 'why', 'ain', \"it's\", 'isn', 'off', 'only', 'ourselves', 'which', 'to', 'she', 'during', \"wouldn't\", 'have', 'if', 'against', 'some', 'for', 'i', \"don't\", \"didn't\", 'ours', 'itself', 'should', 'very', 'those', 'him', 'just', 'll', \"mustn't\", 'out', 'theirs', 'each', 'needn', 'd', 'herself', 'few', 'himself', 'these', 'couldn', 'or', 'not', 'when', 'with', 'does', \"needn't\", \"weren't\", 'into', 'm', 'y', 'we', \"wasn't\", 'both', \"doesn't\", 'you', 'by', 'didn', 'our', 'was', 'wouldn', \"haven't\", 'more', 'who', 'same', 't', 'a', 'hadn', 'what', 'won', \"couldn't\", 'nor', 'they', 'shouldn', 'this', 'there', 'an', 'where', 'while', 'shan', 'has', 'yourself', 'so', 'had', 'myself', 'do', 'been', 'all', 'his', 'haven', 'between', \"you've\", 'doesn', 'at', 'me', 'then', 'until', 'above', 'ma', 'further', 'and', 'of', 're', 'o', 'on', 'up', 'its', 'her', 've', 'because', \"isn't\", 'than', \"shan't\", 'are', 'now', 'is', 'yours', 'will', 'be', 'mightn', 'their', 'being', 'about', 'your', \"won't\", 'don', \"should've\", 'wasn', 'how', 'am', 'weren', 'were', \"hasn't\", 'under', 'as', 'can', \"you'd\", 'from', 'once', \"you'll\", 'doing', 'too', 'down', \"mightn't\", 'before', 'themselves', \"aren't\", 'having', 'aren', 'no', 'over', 'them', 'through', 'but', \"shouldn't\", 'after', 's', 'hers', 'below', 'in', 'again', 'most', 'own', 'hasn', 'my', 'the', 'that', 'other', 'yourselves', 'such', 'any', \"she's\", 'here', \"you're\", \"hadn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idc9_word_list=list(IDC9_filter.toPandas().IDC_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=alphabet_low+stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define python -> spark sql function to remove stop words and find description words\n",
    "def stopword_remove(word_list):\n",
    "    cleaned=[w for w in word_list if not w in stop_list if len(w)>2]\n",
    "    code_descriptions=[w for w in cleaned if  w in idc9_word_list ]\n",
    "    return code_descriptions\n",
    "spark_stop_remove = psf.udf(lambda text: stopword_remove(text), ArrayType(StringType()))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean to just word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_admit_clean=note_admit_label.select('*',spark_cleaner(psf.col(\"TEXT\")).alias(\"TEXT_clean\")).cache()\n",
    "#.limit(1).take(1)[0]['<lambda>(TEXT)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"TEXT_clean\", outputCol=\"tokens\")\n",
    "note_admit_token = tokenizer.transform(note_admit_clean).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51113"
     ]
    }
   ],
   "source": [
    "# run to cache\n",
    "note_admit_token.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stops \n",
    "note_admit_stop = note_admit_token.select('*',spark_stop_remove(psf.col(\"tokens\")).alias(\"tokens_clean\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(tokens_clean=['date', 'discharge', 'date', 'date', 'birth', 'sex', 'medicine', 'patient', 'drugs', 'complaint', 'nausea', 'major', 'surgical', 'procedure', 'central', 'history', 'present', 'illness', 'old', 'ischemic', 'severe', 'type', 'nausea', 'hypotension', 'heart', 'failure', 'clinic', 'kidneys', 'severe', 'renal', 'artery', 'orthopnea', 'vomiting', 'diarrhea', 'recent', 'use', 'decreased', 'initial', 'insulin', 'blood', 'urine', 'central', 'past', 'medical', 'history', 'ischemic', 'severe', 'type', 'blindness', 'complete', 'occlusion', 'cri', 'renal', 'social', 'history', 'alone', 'smoking', 'alcohol', 'use', 'family', 'history', 'non', 'physical', 'exam', 'grade', 'sem', 'diastolic', 'around', 'iliac', 'fossa', 'evidence', 'abscess', 'right', 'lung', 'base', 'pneumonia', 'atelectasis', 'clinical', 'bilateral', 'pleural', 'right', 'left', 'stomach', 'gastroparesis', 'vascular', 'transplant', 'kidney', 'right', 'lower', 'brief', 'course', 'old', 'type', 'history', 'vessel', 'coronary', 'disease', 'cath', 'ischemic', 'cardiomyopathy', 'echo', 'aortic', 'stenosis', 'renal', 'transplant', 'hypotension', 'nausea', 'pta', 'heart', 'failure', 'clinic', 'insulin', 'central', 'blood', 'pressure', 'support', 'per', 'swan', 'catheter', 'hypotension', 'initial', 'swan', 'elevated', 'pap', 'cardiogenic', 'shock', 'ventricular', 'overload', 'echo', 'changes', 'degree', 'stenosis', 'diuretic', 'therapy', 'chf', 'chest', 'pulmonary', 'edema', 'renal', 'elevated', 'status', 'post', 'renal', 'transplant', 'elevated', 'back', 'normal', 'discharge', 'therapy', 'per', 'adequate', 'renal', 'patient', 'stable', 'blood', 'pressure', 'breathing', 'air', 'swan', 'catheter', 'pap', 'floor', 'blood', 'pressure', 'stable', 'blood', 'amp', 'insulin', 'sliding', 'regular', 'hematocrit', 'per', 'renal', 'secondary', 'renal', 'disease', 'patient', 'one', 'blood', 'discharge', 'blood', 'old', 'type', 'diabetic', 'male', 'vessel', 'ischemic', 'cardiomyopathy', 'aortic', 'stenosis', 'hypotension', 'blood', 'pressure', 'hypotension', 'respiratory', 'distress', 'cardiogenic', 'shock', 'support', 'chf', 'blood', 'pressure', 'breathing', 'air', 'sign', 'volume', 'overload', 'physical', 'exam', 'infectious', 'cardiovascular', 'three', 'vessel', 'disease', 'aspirin', 'beta', 'pump', 'aortic', 'stenosis', 'chf', 'class', 'status', 'post', 'cardiogenic', 'shock', 'secondary', 'blood', 'pressure', 'avoid', 'renal', 'disease', 'pressure', 'follow', 'heart', 'failure', 'rhythm', 'low', 'per', 'patient', 'risk', 'aspirin', 'need', 'physical', 'therapy', 'renal', 'disease', 'transplant', 'follow', 'patient', 'primary', 'renal', 'anemia', 'status', 'post', 'transfusion', 'discharge', 'monitor', 'transfusion', 'transfusion', 'prevent', 'volume', 'overload', 'diabetes', 'type', 'renal', 'insufficiency', 'renal', 'transplant', 'need', 'sliding', 'diabetic', 'cardiac', 'healthy', 'fluid', 'hep', 'full', 'physical', 'therapy', 'rehabilitation', 'patient', 'insulin', 'discharge', 'aspirin', 'delayed', 'sig', 'one', 'delayed', 'daily', 'daily', 'sig', 'one', 'daily', 'daily', 'sig', 'one', 'daily', 'daily', 'modified', 'sig', 'sig', 'sig', 'one', 'sig', 'one', 'daily', 'daily', 'sig', 'one', 'sig', 'one', 'rectal', 'modified', 'capsule', 'sig', 'one', 'capsule', 'sig', 'one', 'injection', 'injection', 'sig', 'one', 'appl', 'topical', 'rash', 'sig', 'four', 'insomnia', 'delayed', 'sig', 'one', 'delayed', 'heparin', 'sig', 'one', 'injection', 'injection', 'insulin', 'human', 'insulin', 'insulin', 'use', 'sliding', 'per', 'capsule', 'sig', 'sig', 'one', 'injection', 'sig', 'one', 'injection', 'nausea', 'sig', 'one', 'discharge', 'care', 'facility', 'discharge', 'cardiogenic', 'shock', 'heart', 'failure', 'ischemic', 'cardiomyopathy', 'aortic', 'stenosis', 'coronary', 'artery', 'disease', 'three', 'vessel', 'disease', 'diabetes', 'type', 'peripheral', 'status', 'post', 'kidney', 'transplant', 'discharge', 'condition', 'breathing', 'air', 'blood', 'pressure', 'stable', 'symptoms', 'dizziness', 'nausea', 'chest', 'pain', 'able', 'work', 'physical', 'therapy', 'rehabilitation', 'exercises', 'discharge', 'chest', 'pain', 'shortness', 'breath', 'palpitations', 'light', 'like', 'hypotensive', 'patient', 'rehabilitation', 'facility', 'follow', 'care', 'heart', 'failure'])]"
     ]
    }
   ],
   "source": [
    "note_admit_stop.select('tokens_clean').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HADM_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'NEXT_ADMISSION', 'NEXT_ADMISSION_TYPE', 'days_next_admit', 'CATEGORY', 'TEXT', 'row_number', 'label', 'TEXT_clean', 'tokens', 'tokens_clean']"
     ]
    }
   ],
   "source": [
    "note_admit_stop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lists and columns not fed to create features\n",
    "note_admit_stop_reduce=note_admit_stop.select(note_admit_stop.columns[:10]+['tokens_clean','label']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "note_admit_stop_reduce.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "note_admit_stop.write.parquet('wasb:///final_idc9_filt.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sometimes Azure lags here A quick fix was writing then reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_idc9_filt = spark.read.parquet('wasb:///final_idc9_filt.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parquet this is the base \n",
    "final_idc9_filt.repartition(numPartitions=1).write.parquet('wasb:///final_tokens_with_text.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HADM_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'NEXT_ADMISSION', 'NEXT_ADMISSION_TYPE', 'days_next_admit', 'CATEGORY', 'TEXT', 'row_number', 'label', 'TEXT_clean', 'tokens', 'tokens_clean']"
     ]
    }
   ],
   "source": [
    "final_idc9_filt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if from scratch use 'note_admit_stop'\n",
    "final_data=final_idc9_filt.select(final_idc9_filt.columns[:10]+['tokens_clean','label']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep\n",
    "# final_data.repartition(numPartitions=1).write.parquet('wasb:///final_data_V2.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
